{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUpRxZKeOetKSv5pTnXn5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishu7777/datascience/blob/main/PHASE%202%20DS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn-YWrIv_KCx"
      },
      "outputs": [],
      "source": [
        "Data Preprocessing\n",
        "\n",
        "STEP :1\n",
        "‚óè\timport pandas as pd\n",
        "‚óè\t# Load the dataset\n",
        "‚óè\tdf = pd.read_csv(\"fake_or_real_news.csv\")  # Replace with your actual filename\n",
        "‚óè\tdf.head().]\n",
        "‚óè\tSTEP 2:\n",
        "‚óè\t# Check for missing values\n",
        "‚óè\tprint(df.isnull().sum())\n",
        "‚óè\tSTEP 3:\n",
        "‚óè\t# Check for duplicates\n",
        "‚óè\tduplicates = df.duplicated().sum()\n",
        "‚óè\tprint(f\"Duplicate rows: {duplicates}\")\n",
        "‚óè\n",
        "‚óè\t# Remove duplicates\n",
        "‚óè\tdf = df.drop_duplicates()\n",
        "‚óè\n",
        "‚óè\t# Drop rows with too many missing values (if any)\n",
        "‚óè\tdf = df.dropna(subset=[\"title\", \"text\", \"label\"])  # drop rows missing critical info\n",
        "‚óè\n",
        "‚óè\t# Optionally fill missing with placeholders or impute if needed\n",
        "‚óè\tdf['author'] = df['author'].fillna(\"Unknown\")\n",
        "‚óè\tSTEP 4:\n",
        "‚óè\t# If you have numerical features like 'readability_score' or 'num_words', detect outliers\n",
        "‚óè\timport seaborn as sns\n",
        "‚óè\timport matplotlib.pyplot as plt\n",
        "‚óè\n",
        "‚óè\t# Example for a feature like 'text_length'\n",
        "‚óè\tdf['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "‚óè\n",
        "‚óè\t# Boxplot to detect outliers\n",
        "‚óè\tsns.boxplot(x=df['text_length'])\n",
        "‚óè\tplt.show()\n",
        "‚óè\n",
        "‚óè\t# Remove outliers using IQR method\n",
        "‚óè\tQ1 = df['text_length'].quantile(0.25)\n",
        "‚óè\tQ3 = df['text_length'].quantile(0.75)\n",
        "‚óè\tIQR = Q3 - Q1\n",
        "‚óè\n",
        "‚óè\tdf = df[(df['text_length'] >= Q1 - 1.5 * IQR) & (df['text_length'] <= Q3 + 1.5 * IQR)]\n",
        "‚óè\tSTEP 5:\n",
        "‚óè\t# Convert date columns to datetime, if present\n",
        "‚óè\t# df['date'] = pd.to_datetime(df['date'])\n",
        "‚óè\n",
        "‚óè\t# Ensure labels are strings\n",
        "‚óè\tdf['label'] = df['label'].astype(str)\n",
        "‚óè\tSTEP 6: # Label Encoding (if binary labels like 'fake' and 'real')\n",
        "‚óè\tdf['label_encoded'] = df['label'].map({'fake': 0, 'real': 1})\n",
        "‚óè\n",
        "‚óè\t# One-Hot Encoding (for multiple categories, e.g., news sources)\n",
        "‚óè\t# pd.get_dummies(df['source'], prefix='src')\n",
        "‚óè\tSTEP 7:\n",
        "‚óè\tfrom sklearn.preprocessing import MinMaxScaler\n",
        "‚óè\n",
        "‚óè\t# Normalize 'text_length' for example\n",
        "‚óè\tscaler = MinMaxScaler()\n",
        "‚óè\tdf['text_length_scaled'] = scaler.fit_transform(df[['text_length']])\n",
        "‚óè\tSTEP 8:\n",
        "‚óè\t# Display cleaned dataset info\n",
        "‚óè\tprint(df.info())\n",
        "‚óè\tdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory Data Analysis (EDA)\n",
        "STEP 1: import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add a new column for word count\n",
        "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Histogram of text length\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['text_length'], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Text Length\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "STEP 2:\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x='label', y='text_length', data=df)\n",
        "plt.title(\"Text Length by Label (Fake vs Real)\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "plt.show()\n",
        "STEP 3# sns.pairplot(df[['text_length', 'label_encoded', 'other_numeric_feature']], hue='label'):\n",
        "STEP 4: plt.figure(figsize=(8,5))\n",
        "sns.violinplot(x='label', y='text_length', data=df)\n",
        "plt.title(\"Violin Plot: Text Length by Label\")\n",
        "plt.show()\n",
        "STEP 5: # Assuming you have a 'sentiment' column\n",
        "# sns.barplot(x='label', y='sentiment_score', data=df)\n"
      ],
      "metadata": {
        "id": "nXf12cJD_Txh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Create New Features Based on Domain Knowledge or EDA\n",
        "üìå Add text_length and title_length\n",
        "python\n",
        "CopyEdit\n",
        "# Length of text and title\n",
        "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "df['title_length'] = df['title'].apply(lambda x: len(x.split()))\n",
        "Justification: Fake news may use shorter, attention-grabbing titles and verbose or sensational content. These length features help capture writing style.\n",
        "________________________________________\n",
        "üìå Add punctuation_count and uppercase_ratio\n",
        "python\n",
        "CopyEdit\n",
        "df['punctuation_count'] = df['text'].apply(lambda x: sum([1 for c in x if c in \".,!?;:\"]))\n",
        "df['uppercase_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
        "Justification: Fake articles often contain excessive punctuation or uppercase letters to express urgency or emotion.\n",
        "________________________________________\n",
        "üîÑ 2. Combine or Split Columns\n",
        "üìå Extract Date Parts (if date column exists)\n",
        "python\n",
        "CopyEdit\n",
        "# If there's a 'date' column\n",
        "# df['date'] = pd.to_datetime(df['date'])\n",
        "# df['year'] = df['date'].dt.year\n",
        "# df['month'] = df['date'].dt.month\n",
        "# df['dayofweek'] = df['date'].dt.dayofweek\n",
        "Justification: Temporal trends may reveal fake news clustering in specific months or days (e.g., elections).\n",
        "________________________________________\n",
        "üìä 3. Apply Binning / Grouping\n",
        "üìå Bin text length into categories\n",
        "python\n",
        "CopyEdit\n",
        "df['text_length_bin'] = pd.cut(df['text_length'], bins=[0, 300, 600, 900, 1200, np.inf], labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long'])\n",
        "Justification: Helps tree-based models or visualizations by grouping similar-sized articles.\n",
        "________________________________________\n",
        "‚ú® 4. Polynomial and Interaction Features (for advanced models)\n",
        "üìå Add interaction between title and text lengths\n",
        "python\n",
        "CopyEdit\n",
        "df['length_ratio'] = df['title_length'] / (df['text_length'] + 1)\n",
        "Justification: Fake news may have disproportionately short titles compared to body length, highlighting clickbait patterns.\n",
        "________________________________________\n",
        "üîª 5. Dimensionality Reduction (Optional)\n",
        "üìå Apply PCA to TF-IDF vectors\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
        "Justification: PCA reduces high-dimensional TF-IDF space, speeding up training and reducing overfitting.\n",
        "________________________________________\n",
        "‚úÖ Final Justification Summary\n",
        "Feature\tPurpose\tJustification\n",
        "text_length\tNumerical\tDetect article verbosity\n",
        "title_length\tNumerical\tSpot clickbait headlines\n",
        "punctuation_count\tStyle indicator\tExcess punctuation = urgency/sensationalism\n",
        "uppercase_ratio\tTone indicator\tUppercase overuse = emotional manipulation\n",
        "length_ratio\tInteraction\tTitle-to-body ratio reflects content quality\n",
        "text_length_bin\tCategorical\tHelps grouping similar articles\n",
        "PCA components\tDimensionality\tCompresses TF-IDF without losing major variance\n",
        "\n",
        "8. Model Building\n",
        "Target: label_encoded (0 = Fake, 1 = Real)\n",
        "________________________________________\n",
        "‚úÖ Step 1: Import Required Libraries\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "________________________________________\n",
        "üî† Step 2: Feature Preparation (TF-IDF on Text)\n",
        "python\n",
        "CopyEdit\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label_encoded']\n",
        "________________________________________\n",
        "‚úÇÔ∏è Step 3: Train-Test Split (with Stratification)\n",
        "python\n",
        "CopyEdit\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "________________________________________\n",
        "ü§ñ Step 4: Model 1 ‚Äì Logistic Regression\n",
        "python\n",
        "CopyEdit\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lr = logreg.predict(X_test)\n",
        "________________________________________\n",
        "üå≥ Model 2 ‚Äì Random Forest Classifier\n",
        "python\n",
        "CopyEdit\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "________________________________________\n",
        "üìä Step 5: Evaluation Metrics Function\n",
        "python\n",
        "CopyEdit\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score:  {f1_score(y_true, y_pred):.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Fake', 'Real']))\n",
        "________________________________________\n",
        "üìà Step 6: Evaluate Both Models\n",
        "python\n",
        "CopyEdit\n",
        "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\n",
        "evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
        "________________________________________\n",
        "üß† Justification of Model Choice\n",
        "Model\tReason for Selection\n",
        "Logistic Regression\tA simple, interpretable baseline for binary classification, fast and efficient on sparse TF-IDF data.\n",
        "Random Forest\tA powerful ensemble method that handles non-linear patterns and gives feature importance. Robust to overfitting.\n",
        "________________________________________\n",
        "üîç Example Output Summary (Hypothetical)\n",
        "Model\tAccuracy\tPrecision\tRecall\tF1-score\n",
        "Logistic Regression\t0.93\t0.91\t0.94\t0.92\n",
        "Random Forest\t0.96\t0.95\t0.93\t0.92\n",
        "\n",
        "\n",
        "9. Visualization of Results & Model Insights\n",
        ".] 1. Confusion Matrix (Already Done Earlier)\n",
        "What it shows:\n",
        "A summary of prediction results:\n",
        "‚Ä¢\tTP (True Positive): Real news predicted as real.\n",
        "‚Ä¢\tTN (True Negative): Fake news predicted as fake.\n",
        "‚Ä¢\tFP (False Positive): Fake predicted as real.\n",
        "‚Ä¢\tFN (False Negative): Real predicted as fake.\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Repeat for XGBoost\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, display_labels=['Fake', 'Real'], cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - XGBoost\")\n",
        "plt.show()\n",
        "üîç Conclusion:\n",
        "Fewer off-diagonal values = better performance. High TP and TN indicate strong predictive accuracy.\n",
        "________________________________________\n",
        "üìà 2. ROC Curve and AUC Score\n",
        "What it shows:\n",
        "ROC (Receiver Operating Characteristic) curve plots the trade-off between true positive rate (sensitivity) and false positive rate.\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = xgb.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0,1], [0,1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "üîç Conclusion:\n",
        "Higher AUC (Area Under Curve) means better separability of fake vs real news. AUC near 1.0 = excellent classifier.\n",
        "________________________________________\n",
        "üå≥ 3. Feature Importance Plot (XGBoost or Random Forest)\n",
        "What it shows:\n",
        "Top features the model used to make decisions. Useful for interpretability.\n",
        "python\n",
        "CopyEdit\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# For XGBoost\n",
        "importances = xgb.feature_importances_\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "top_indices = np.argsort(importances)[-20:][::-1]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=importances[top_indices], y=np.array(feature_names)[top_indices])\n",
        "plt.title(\"Top 20 Important Words (XGBoost)\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Word\")\n",
        "plt.show()\n",
        "üîç Conclusion:\n",
        "Words like \"breaking\", \"shocking\", \"election\", or emotionally charged terms are often top predictors in fake news.\n",
        "________________________________________\n",
        "üìä 4. Visual Comparison of Model Performance\n",
        "python\n",
        "CopyEdit\n",
        "import pandas as pd\n",
        "\n",
        "# Example metrics (replace with actual values)\n",
        "metrics = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
        "    'Accuracy': [0.93, 0.96, 0.97],\n",
        "    'F1-Score': [0.92, 0.96, 0.97]\n",
        "})\n",
        "\n",
        "metrics.set_index('Model')[['Accuracy', 'F1-Score']].plot(kind='bar', figsize=(8,5), colormap='viridis')\n",
        "plt.title('Model Comparison: Accuracy & F1 Score')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0.85, 1.0)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1DTXSb5B_ibH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}