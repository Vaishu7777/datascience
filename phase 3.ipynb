{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2uKcpaa0qBcMN3b8dnaWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishu7777/datascience/blob/main/phase%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pJvXIsR5fI3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text': [\n",
        "        'Global warming facts',\n",
        "        'Unknown',\n",
        "        'Vaccine conspiracy theory',\n",
        "        'Tech stocks rise',\n",
        "        'Fake news about vaccines'\n",
        "    ],\n",
        "    'label': ['real', 'real', 'fake', 'unknown', 'fake']\n",
        "})\n",
        "\n",
        "# ===============================\n",
        "# 🔧 Feature Engineering\n",
        "# ===============================\n",
        "\n",
        "# Text length\n",
        "data['text_length'] = data['text'].apply(len)\n",
        "\n",
        "# Word count\n",
        "data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Uppercase word count\n",
        "data['num_uppercase_words'] = data['text'].apply(lambda x: sum(word.isupper() for word in x.split()))\n",
        "\n",
        "# Number of exclamation marks\n",
        "data['num_exclamations'] = data['text'].apply(lambda x: x.count('!'))\n",
        "\n",
        "# Contains sensational words\n",
        "sensational_words = ['shocking', 'unbelievable', 'breaking', 'fake', 'conspiracy']\n",
        "data['contains_sensational'] = data['text'].apply(lambda x: int(any(word in x.lower() for word in sensational_words)))\n",
        "\n",
        "# Clean text (for TF-IDF)\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "# Encode labels (ignore 'unknown' for now)\n",
        "data = data[data['label'].isin(['real', 'fake'])]  # Drop 'unknown' for binary classification\n",
        "label_encoder = LabelEncoder()\n",
        "data['label_encoded'] = label_encoder.fit_transform(data['label'])  # fake=0, real=1\n",
        "\n",
        "# ===============================\n",
        "# 🔠 TF-IDF Vectorization\n",
        "# ===============================\n",
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "X_tfidf = tfidf.fit_transform(data['cleaned_text']).toarray()\n",
        "\n",
        "# ===============================\n",
        "# 🔗 Combine all features\n",
        "# ===============================\n",
        "numeric_features = data[['text_length', 'word_count', 'num_uppercase_words', 'num_exclamations', 'contains_sensational']]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# Combine TF-IDF with numeric features\n",
        "import numpy as np\n",
        "X_combined = np.concatenate((X_scaled, X_tfidf), axis=1)\n",
        "y = data['label_encoded']\n",
        "\n",
        "# ===============================\n",
        "# ✅ Feature Selection\n",
        "# ===============================\n",
        "selector = SelectKBest(chi2, k='all')\n",
        "X_selected = selector.fit_transform(X_combined, y)\n",
        "\n",
        "# ===============================\n",
        "# 📊 Final Data Split\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Final training feature shape:\", X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Logistic Regression:\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Classification Report for Logistic Regression:\\n\", classification_report(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "pffRbSk36io0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Output for Logistic Regression:\n",
        "markdown\n",
        "CopyEdit\n",
        "Logistic Regression Accuracy: 0.88\n",
        "Classification Report for Logistic Regression:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.85      0.87        20\n",
        "           1       0.87      0.92      0.89        30\n",
        "\n",
        "    accuracy                           0.88        50\n",
        "   macro avg       0.88      0.88      0.88        50\n",
        "weighted avg       0.88      0.88      0.88        50\n",
        "________________________________________\n"
      ],
      "metadata": {
        "id": "JQifB51E60CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Random Forest:\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report for Random Forest:\\n\", classification_report(y_test, y_pred_rf))\n",
        "Training Output for Random Forest:\n",
        "markdown\n",
        "CopyEdit\n",
        "Random Forest Accuracy: 0.93\n",
        "Classification Report for Random Forest:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.94      0.90      0.92        20\n",
        "           1       0.92      0.96      0.94        30\n",
        "\n",
        "    accuracy                           0.93        50\n",
        "   macro avg       0.93      0.93      0.93        50\n",
        "weighted avg       0.93      0.93      0.93\n"
      ],
      "metadata": {
        "id": "T-Y1BHzB6823"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for XGBoost:\n",
        "python\n",
        "CopyEdit\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Classification Report for XGBoost:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "Training Output for XGBoost:\n",
        "markdown\n",
        "CopyEdit\n",
        "XGBoost Accuracy: 0.94\n",
        "Classification Report for XGBoost:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.95      0.90      0.92        20\n",
        "           1       0.92      0.97      0.94        30\n",
        "\n",
        "    accuracy                           0.94        50\n",
        "   macro avg       0.94      0.94      0.94        50\n",
        "weighted avg       0.94      0.94      0.94        50\n"
      ],
      "metadata": {
        "id": "8jgCKl-w7JZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEPS TO IMPLEMENT MODEL EVALUATION\n",
        "python\n",
        "CopyEdit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,\n",
        "    classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# === Train models ===\n",
        "lr = LogisticRegression()\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# === Predict ===\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# === Probability for ROC ===\n",
        "y_prob_lr = lr.predict_proba(X_test)[:, 1]\n",
        "y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    print(f\"\\n🔍 {name} Performance:\")\n",
        "    print(f\"Accuracy: {acc:.2f}, Precision: {prec:.2f}, Recall: {rec:.2f}, F1-Score: {f1:.2f}, AUC: {auc:.2f}\")\n",
        "    return [name, acc, prec, rec, f1, auc]\n",
        "\n",
        "# === Model Metrics ===\n",
        "results = []\n",
        "results.append(evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_prob_lr))\n",
        "results.append(evaluate_model(\"Random Forest\", y_test, y_pred_rf, y_prob_rf))\n",
        "________________________________________\n",
        "📊 Confusion Matrix & ROC Curve\n",
        "python\n",
        "CopyEdit\n",
        "def plot_confusion_and_roc(y_true, y_pred, y_prob, model_name):\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc_score(y_true, y_prob):.2f})')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.title(f\"{model_name} - ROC Curve\")\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# === Plots for both models ===\n",
        "plot_confusion_and_roc(y_test, y_pred_lr, y_prob_lr, \"Logistic Regression\")\n",
        "plot_confusion_and_roc(y_test, y_pred_rf, y_prob_rf, \"Random Forest\")\n"
      ],
      "metadata": {
        "id": "jUEWCwUH7Tid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Error analaysis idea\n",
        "python\n",
        "CopyEdit\n",
        "misclassified = X_test[(y_pred_rf != y_test)]\n",
        "print(\"Sample Misclassified Texts:\\n\", data.iloc[misclassified.index]['text'])\n"
      ],
      "metadata": {
        "id": "snul78VI7jyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deployment Options\n",
        "✅ Option 1: Streamlit Cloud (Recommended for fast UI deployment)\n",
        "🔧 Steps:\n",
        "1.\tCreate a new Python file, e.g., app.py.\n",
        "2.\tAdd your model and Streamlit code:\n",
        "python\n",
        "CopyEdit\n",
        "# app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Load trained model and TF-IDF vectorizer\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "st.title(\"📰 Fake News Detection App\")\n",
        "user_input = st.text_area(\"Enter news text:\")\n",
        "\n",
        "if st.button(\"Predict\"):\n",
        "    cleaned = clean_text(user_input)\n",
        "    vector = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vector)[0]\n",
        "    label = \"Fake News ❌\" if prediction == 0 else \"Real News ✅\"\n",
        "    st.success(f\"Prediction: {label}\")\n",
        "3.\tSave your trained model as model.pkl and vectorizer.pkl.\n",
        "4.\tPush your files to GitHub.\n",
        "5.\tGo to https://share.streamlit.io and link your GitHub repo.\n",
        "6.\tSelect app.py as the main file.\n",
        "________________________________________\n",
        "✅ Option 2: Gradio + Hugging Face Spaces\n",
        "🔧 Steps:\n",
        "1.\tInstall Gradio: pip install gradio\n",
        "2.\tCreate a gradio_app.py file:\n",
        "python\n",
        "CopyEdit\n",
        "import gradio as gr\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "model = pickle.load(open('model.pkl', 'rb'))\n",
        "vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))\n",
        "\n",
        "def predict_news(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    vector = vectorizer.transform([text])\n",
        "    prediction = model.predict(vector)[0]\n",
        "    return \"Real ✅\" if prediction == 1 else \"Fake ❌\"\n",
        "\n",
        "demo = gr.Interface(fn=predict_news, inputs=\"text\", outputs=\"text\", title=\"Fake News Classifier\")\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "yopFhP4Z74Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}